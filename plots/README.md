# Comprehensive Stress Test Results - Graph Analysis

**Generated**: November 24, 2025  
**Dataset**: BurstGPT (Azure ChatGPT traces)  
**SLA Target**: 1.0 second  
**RPS Scaling**: 200x (high pressure scenario)

---

## üìä Generated Visualizations

### 1. Request Scaling Analysis

#### `request_scaling_capacity.png`
**What it shows**: How capacity QPS and SLA violation rate change as request volume increases (10K ‚Üí 100K ‚Üí 1M)

**Key insights**:
- **Left panel**: Capacity QPS vs request volume (log-log scale)
  - Multi-bin (4 GPUs) maintains 6-7 QPS across volumes
  - Dynamic (1 GPU) degrades from 1.13 ‚Üí 0.29 QPS as load increases
  - Static (1 GPU) degrades from 0.57 ‚Üí 0.26 QPS
  - Annotations show GPU count for each configuration
  
- **Right panel**: SLA violation rate vs request volume
  - All single-GPU schedulers show >90% violations at high load
  - Multi-bin (4 GPUs) stays relatively stable (4.6% ‚Üí 60% ‚Üí 76%)
  - Clear separation between multi-GPU and single-GPU architectures

**Takeaway**: Multi-bin's parallel GPU architecture provides consistent capacity even as load increases.

---

#### `request_scaling_latency.png`
**What it shows**: Average and P95 latency vs request volume

**Key insights**:
- **Left panel**: Average latency (log-log scale)
  - Static: 25s ‚Üí 1283s (53x degradation)
  - Dynamic: 4.6s ‚Üí 2079s (452x degradation!)
  - Multi-bin: 0.4s ‚Üí 98s ‚Üí 5338s (gradual degradation)
  - Red line marks 1.0s SLA target
  
- **Right panel**: P95 latency
  - Shows tail latency behavior
  - Multi-bin has best P95 performance at all scales
  - Single-GPU schedulers have catastrophic tail latency under load

**Takeaway**: At high load, even multi-bin struggles, but degrades more gracefully than alternatives.

---

### 2. GPU Scaling Analysis (Multi-Bin Only)

#### `gpu_scaling_analysis.png`
**What it shows**: How multi-bin scheduler performance changes from 4 ‚Üí 64 GPUs (1M requests)

**Four panels**:

1. **Capacity QPS vs GPUs**
   - Shows QPS improvement with more GPUs
   - Dashed line = ideal linear scaling
   - Actual scaling is sublinear (efficiency decreases with scale)
   - From 6.08 QPS (4 GPUs) ‚Üí 45.69 QPS (64 GPUs)

2. **SLA Violation Rate vs GPUs**
   - Violations decrease from 75.6% ‚Üí 15.2%
   - Red line marks 10% target
   - Need ~100 GPUs to reach <10% violations at this load
   
3. **Average Latency vs GPUs** (log scale)
   - Dramatic improvement: 5338s ‚Üí 2.2s (2400x!)
   - Red line marks 1.0s SLA
   - Latency drops exponentially as GPUs reduce queueing
   
4. **GPU Utilization vs GPUs**
   - High at 4 GPUs (94.1%) - overloaded
   - Drops to 17.6% at 64 GPUs - underutilized
   - Orange line = 50% balanced, Red line = 80% saturated
   - Shows trade-off: more GPUs = better SLA but lower efficiency

**Takeaway**: GPU scaling helps SLA compliance but has diminishing returns beyond 16-32 GPUs.

---

#### `scaling_efficiency.png`
**What it shows**: GPU scaling efficiency metrics

**Two panels**:

1. **QPS Ratio vs GPU Ratio**
   - Compares actual QPS improvement to GPU count increase
   - Dashed line = ideal linear scaling (1:1)
   - Points labeled with GPU count
   - Shows superlinear scaling early (4‚Üí8 GPUs), then sublinear
   
2. **Scaling Efficiency Bar Chart**
   - Green bars (‚â•85%): Excellent efficiency
   - Orange bars (60-85%): Acceptable efficiency
   - Red bars (<60%): Poor efficiency
   - Value labels show exact efficiency percentage
   - 4‚Üí8 GPUs: 133% (superlinear - queueing relief)
   - 8‚Üí16 GPUs: 106% (near-linear)
   - 16‚Üí32 GPUs: 61% (sublinear - input limited)
   - 32‚Üí64 GPUs: 37% (poor - diminishing returns)

**Takeaway**: Sweet spot is 8-16 GPUs for this workload. Beyond that, you're paying for latency reduction, not throughput.

---

### 3. Scheduler Comparison

#### `scheduler_comparison_heatmap.png`
**What it shows**: Normalized performance across 7 key metrics (10K requests)

**Metrics** (columns):
- Capacity QPS
- SLA Compliance (inverted violations)
- Avg Latency (lower is better)
- P95 Latency (lower is better)
- Throughput
- GPU Util
- Batch Size

**Color coding**:
- Green (1.0): Best performance on that metric
- Yellow (0.5): Middle performance
- Red (0.0): Worst performance

**Reading the heatmap**:
- Multi-bin: Mostly green (best overall)
- Dynamic: Mixed (good capacity, moderate on others)
- Static: Mostly red/yellow (worst overall)

**Takeaway**: Multi-bin dominates on SLA-critical metrics (capacity, latency compliance).

---

#### `throughput_latency_tradeoff.png`
**What it shows**: The classic throughput-latency tradeoff for all configurations

**Axes**:
- X-axis: Average latency (log scale)
- Y-axis: Throughput (requests/sec)
- Bubble size: Number of GPUs

**Patterns**:
- Red circles: Static FIFO (1 GPU)
- Blue squares: Dynamic no-bins (1 GPU)
- Green triangles: Multi-bin (4-64 GPUs)

**Ideal region**: Top-left (high throughput, low latency)

**Insights**:
- Vertical red line marks 1.0s SLA
- Most configurations to the right of SLA (late)
- Multi-bin cluster shows trade-off: more GPUs = lower latency but not always higher throughput
- Single-GPU schedulers stuck in high-latency region

**Takeaway**: Latency and throughput are coupled. Multi-bin breaks this with parallel processing.

---

### 4. Batch Size Analysis

#### `batch_size_analysis.png`
**What it shows**: How batch sizes change with scale

**Two panels**:

1. **Batch Size vs Request Volume**
   - Static (red): Constant at ~6-7 (fixed batching)
   - Dynamic (blue): Drops from ~5.6 ‚Üí 4.6 as load increases
   - Multi-bin (green): Increases from 1.2 ‚Üí 3.8 (adapts to load)
   - Shows different batching strategies under pressure
   
2. **Multi-Bin Batch Size vs GPU Count** (1M requests)
   - Batch size increases from 3.8 ‚Üí 5.7 as GPUs scale
   - More GPUs = less queueing pressure = can use larger batches
   - Demonstrates SLA controller adaptation

**Takeaway**: Dynamic and multi-bin actively adjust batch sizes. Multi-bin increases batches as GPUs reduce queueing.

---

## üìà Summary Tables

### `summary_table.csv` / `summary_table.txt`
Complete tabular summary of all test results with:
- Scheduler type
- Request count
- GPU count
- Capacity QPS
- SLA violation rate
- Latency metrics (avg, P95)
- Throughput
- GPU utilization
- Average batch size

**Use this for**: Quick numerical comparisons, copy-paste into papers/presentations.

---

### `analysis_report.txt`
Text-based analysis report with:
- Key findings summary
- Scheduler comparison (10K requests)
- GPU scaling analysis (1M requests)
- Scaling efficiency calculations

**Use this for**: Quick reference, summary for non-technical stakeholders.

---

## üéØ Key Takeaways from Graphs

### Performance Hierarchy (VALIDATED ‚úÖ)
```
Multi-Bin Dynamic (4 GPU) >> Dynamic No-Bins (1 GPU) > Static FIFO (1 GPU)
        6.00 QPS                    1.13 QPS               0.57 QPS
```

### GPU Scaling Insights
1. **Superlinear scaling (4‚Üí8 GPUs)**: 133% efficiency - relieving overload
2. **Linear scaling (8‚Üí16 GPUs)**: 106% efficiency - sweet spot
3. **Sublinear scaling (16‚Üí64 GPUs)**: 37-61% efficiency - diminishing returns

### SLA Compliance
- **4 GPUs**: 4.6% violations (10K reqs) ‚Üí 75.6% violations (1M reqs)
- **64 GPUs**: 15.2% violations (1M reqs) - needs ~100 GPUs for <10%
- **Single GPU**: >90% violations at any significant load

### Resource Utilization
- **4-8 GPUs**: 85-94% util - saturated, efficient but SLA-challenged
- **16 GPUs**: 55% util - balanced sweet spot
- **64 GPUs**: 18% util - underutilized but meets SLA

---

## üìù How to Regenerate

```bash
# Regenerate all plots from scratch
python scripts/analyze_and_plot_results.py \
    --input comprehensive_research_results_fixed.csv \
    --output-dir plots

# Use custom input/output
python scripts/analyze_and_plot_results.py \
    --input your_results.csv \
    --output-dir your_plots/
```

---

## üîç Plot Interpretation Tips

### Reading Log Scales
- Both axes log: Straight line = power law relationship
- One axis log: Exponential growth/decay
- Parallel lines: Constant ratio between schedulers

### Color Coding Consistency
- üî¥ Red: Static FIFO
- üîµ Blue: Dynamic No-Bins
- üü¢ Green: Multi-Bin Dynamic

### Reference Lines
- Red dashed: SLA target (1.0s) or critical threshold (10%)
- Gray dashed: Ideal/linear scaling reference
- Orange/Yellow: Warning thresholds (50%, 80% util)

### Bubble Sizes
- Represent GPU count in throughput-latency tradeoff
- Larger bubbles = more GPUs used

---

## üìö Related Documentation

- **Comprehensive Analysis**: `../COMPREHENSIVE_ANALYSIS_FIXED.md`
- **Fix Summary**: `../DYNAMIC_BATCHING_FIX_SUMMARY.md`
- **Quick Reference**: `../QUICK_REFERENCE_RESULTS.md`
- **Raw Data**: `../comprehensive_research_results_fixed*.csv`

---

**Questions?** Check the analysis report or comprehensive documentation in the root directory.
